# SYNTARA-PRO: Complete Examples Repository

## Examples Overview

This repository contains comprehensive examples for SYNTARA-PRO API usage:

### üìÅ Examples Structure
```
examples/
‚îú‚îÄ‚îÄ basic/
‚îÇ   ‚îú‚îÄ‚îÄ hello_world.py          # Simple text generation
‚îÇ   ‚îú‚îÄ‚îÄ neural_processing.py     # Neural data processing
‚îÇ   ‚îî‚îÄ‚îÄ vision_analysis.py      # Image processing
‚îú‚îÄ‚îÄ advanced/
‚îÇ   ‚îú‚îÄ‚îÄ streaming_client.py      # Real-time streaming
‚îÇ   ‚îú‚îÄ‚îÄ batch_processing.py     # Batch requests
‚îÇ   ‚îî‚îÄ‚îÄ multimodal.py          # Multi-modal processing
‚îú‚îÄ‚îÄ bilingual/
‚îÇ   ‚îú‚îÄ‚îÄ hindi_support.py        # Hindi text processing
‚îÇ   ‚îú‚îÄ‚îÄ translation.py          # Translation between languages
‚îÇ   ‚îî‚îÄ‚îÄ code_switching.py      # Mixed language handling
‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îú‚îÄ‚îÄ error_handling.py       # Robust error handling
‚îÇ   ‚îú‚îÄ‚îÄ rate_limiting.py       # Rate limiting implementation
‚îÇ   ‚îî‚îÄ‚îÄ monitoring.py          # Performance monitoring
‚îî‚îÄ‚îÄ integrations/
    ‚îú‚îÄ‚îÄ flask_app.py           # Flask integration
    ‚îú‚îÄ‚îÄ django_app.py          # Django integration
    ‚îî‚îÄ‚îÄ react_client.py        # React frontend
```

---

## Basic Examples

### 1. Hello World (basic/hello_world.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Basic Example: Hello World
Simple text generation example
"""

import requests
import json
import time

def main():
    # API endpoint
    url = "http://localhost:8000/process"
    
    # Request data
    payload = {
        "input_data": "Hello, SYNTARA-PRO! Tell me about your capabilities.",
        "task_type": "text_generation",
        "max_tokens": 150,
        "temperature": 0.7
    }
    
    print("üöÄ Sending request to SYNTARA-PRO...")
    print(f"üìù Input: {payload['input_data']}")
    
    try:
        # Send request
        start_time = time.time()
        response = requests.post(url, json=payload)
        processing_time = time.time() - start_time
        
        # Handle response
        if response.status_code == 200:
            result = response.json()
            
            print(f"\n‚úÖ Success! Processing time: {processing_time:.2f}s")
            print(f"ü§ñ Response: {result['result']}")
            print(f"üìä Metadata: {result['metadata']}")
            
        else:
            print(f"‚ùå Error: {response.status_code}")
            print(f"üìÑ Response: {response.text}")
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Network error: {e}")

if __name__ == "__main__":
    main()
```

### 2. Neural Processing (basic/neural_processing.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Neural Processing Example
Process neural data through spiking networks
"""

import numpy as np
import requests
import matplotlib.pyplot as plt

def generate_neural_data():
    """Generate sample neural data."""
    # Simulate neural activity
    time_points = 1000
    channels = 64
    
    # Create realistic neural patterns
    data = np.random.randn(time_points, channels)
    
    # Add some structure (oscillations)
    for t in range(time_points):
        data[t] += 0.5 * np.sin(2 * np.pi * t / 100)
        data[t] += 0.3 * np.cos(2 * np.pi * t / 50)
    
    return data

def visualize_neural_data(data):
    """Visualize neural activity."""
    plt.figure(figsize=(12, 8))
    
    # Plot first few channels
    for i in range(min(8, data.shape[1])):
        plt.subplot(2, 4, i + 1)
        plt.plot(data[:, i], linewidth=0.5)
        plt.title(f"Channel {i+1}")
    plt.tight_layout()
    plt.savefig('neural_activity.png')
    plt.show()

def main():
    print("üß† SYNTARA-PRO Neural Processing Example")
    
    # Generate neural data
    print("üìä Generating neural data...")
    neural_data = generate_neural_data()
    
    # Visualize
    print("üìà Visualizing neural activity...")
    visualize_neural_data(neural_data)
    
    # Process with SYNTARA-PRO
    print("üöÄ Processing with SYNTARA-PRO...")
    
    url = "http://localhost:8000/process"
    payload = {
        "input_data": neural_data.tolist(),
        "task_type": "neural_processing"
    }
    
    try:
        response = requests.post(url, json=payload)
        if response.status_code == 200:
            result = response.json()
            
            print(f"\n‚úÖ Neural processing complete!")
            print(f"üß† Modules used: {result['metadata']['modules_used']}")
            print(f"‚è±Ô∏è Processing time: {result['metadata']['processing_time']:.3f}s")
            
            # Display results
            if 'spiking_stats' in result['result']:
                stats = result['result']['spiking_stats']
                print(f"üî• Total spikes: {stats.get('total_spikes', 0)}")
                print(f"üß¨ Neurons active: {stats.get('n_neurons', 0)}")
            
        else:
            print(f"‚ùå Error: {response.status_code}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()
```

### 3. Vision Analysis (basic/vision_analysis.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Vision Analysis Example
Process images with computer vision
"""

import requests
import numpy as np
from PIL import Image
import io
import matplotlib.pyplot as plt

def create_test_image():
    """Create a test image for processing."""
    # Create a colorful test pattern
    img_array = np.zeros((224, 224, 3), dtype=np.uint8)
    
    # Add geometric patterns
    for i in range(224):
        for j in range(224):
            # Create gradient
            img_array[i, j, 0] = int(255 * i / 224)  # Red gradient
            img_array[i, j, 1] = int(255 * j / 224)  # Green gradient
            img_array[i, j, 2] = int(255 * (i + j) / 448)  # Blue gradient
    
    # Add some shapes
    img_array[50:100, 50:100] = [255, 255, 255]  # White square
    img_array[150:200, 150:200] = [0, 0, 0]      # Black square
    
    return Image.fromarray(img_array)

def visualize_results(original_image, result):
    """Visualize vision processing results."""
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))
    
    # Original image
    axes[0].imshow(original_image)
    axes[0].set_title("Original Image")
    axes[0].axis('off')
    
    # Results visualization
    if result and 'success' in result and result['success']:
        # Create a simple visualization of features
        features = result.get('result', {}).get('features', [])
        if features:
            feature_array = np.array(features[:100]).reshape(10, 10)
            axes[1].imshow(feature_array, cmap='viridis')
            axes[1].set_title("Extracted Features")
        else:
            axes[1].text(0.5, 0.5, "No features extracted", 
                          ha='center', va='center', transform=axes[1].transAxes)
            axes[1].set_title("Processing Result")
    else:
        axes[1].text(0.5, 0.5, "Processing Failed", 
                      ha='center', va='center', transform=axes[1].transAxes)
        axes[1].set_title("Error")
    
    axes[1].axis('off')
    plt.tight_layout()
    plt.savefig('vision_analysis.png')
    plt.show()

def main():
    print("üëÅÔ∏è SYNTARA-PRO Vision Analysis Example")
    
    # Create test image
    print("üé® Creating test image...")
    test_image = create_test_image()
    test_image.save('test_image.png')
    
    # Convert to bytes for API
    img_bytes = io.BytesIO()
    test_image.save(img_bytes, format='JPEG')
    img_bytes.seek(0)
    
    # Process with SYNTARA-PRO
    print("üöÄ Processing image with SYNTARA-PRO...")
    
    url = "http://localhost:8000/process"
    
    # Send as multipart form data
    files = {'image': ('test_image.jpg', img_bytes, 'image/jpeg')}
    data = {'task_type': 'vision'}
    
    try:
        response = requests.post(url, files=files, data=data)
        if response.status_code == 200:
            result = response.json()
            
            print(f"\n‚úÖ Vision processing complete!")
            print(f"üëÅÔ∏è Modules used: {result['metadata']['modules_used']}")
            print(f"‚è±Ô∏è Processing time: {result['metadata']['processing_time']:.3f}s")
            
            # Display results
            vision_result = result.get('result', {})
            if 'num_patches' in vision_result:
                print(f"üî≤ Image patches: {vision_result['num_patches']}")
            if 'logits' in vision_result:
                top_predictions = vision_result['logits'][:5]
                print(f"üèÜ Top predictions: {top_predictions}")
            
            # Visualize
            visualize_results(test_image, result)
            
        else:
            print(f"‚ùå Error: {response.status_code}")
            print(f"üìÑ Response: {response.text}")
            
    except Exception as e:
        print(f"‚ùå Error: {e}")

if __name__ == "__main__":
    main()
```

---

## Advanced Examples

### 4. Streaming Client (advanced/streaming_client.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Streaming Example
Real-time streaming responses for long generations
"""

import requests
import json
import time
import threading
from queue import Queue

class StreamingClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.response_queue = Queue()
    
    def stream_process(self, input_data, max_tokens=500):
        """Process with streaming response."""
        url = f"{self.base_url}/process"
        
        payload = {
            "input_data": input_data,
            "task_type": "text_generation",
            "max_tokens": max_tokens,
            "stream": True
        }
        
        print(f"üöÄ Starting streaming process...")
        print(f"üìù Input: {input_data[:50]}...")
        
        try:
            response = requests.post(url, json=payload, stream=True)
            
            if response.status_code == 200:
                full_response = ""
                
                for line in response.iter_lines():
                    if line:
                        line_str = line.decode('utf-8')
                        if line_str.startswith('data: '):
                            data_str = line_str[6:]  # Remove 'data: ' prefix
                            
                            if data_str == '[DONE]':
                                print("\n‚úÖ Streaming complete!")
                                break
                            
                            try:
                                data = json.loads(data_str)
                                self.handle_stream_chunk(data)
                                
                                if data['type'] == 'complete':
                                    full_response = data['result']['result']
                                    
                            except json.JSONDecodeError:
                                continue
                
                return full_response
            else:
                print(f"‚ùå Error: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"‚ùå Streaming error: {e}")
            return None
    
    def handle_stream_chunk(self, chunk):
        """Handle individual streaming chunk."""
        chunk_type = chunk.get('type', 'unknown')
        
        if chunk_type == 'start':
            print("üé¨ Processing started...")
            
        elif chunk_type == 'progress':
            step = chunk.get('step', 0)
            total = chunk.get('total', 1)
            message = chunk.get('message', '')
            print(f"‚è≥ Progress: {step}/{total} - {message}")
            
        elif chunk_type == 'complete':
            result = chunk.get('result', {})
            print(f"‚úÖ Final result: {result.get('result', '')[:100]}...")
            
        elif chunk_type == 'error':
            error = chunk.get('error', 'Unknown error')
            print(f"‚ùå Error: {error}")

def main():
    print("üåä SYNTARA-PRO Streaming Example")
    
    client = StreamingClient()
    
    # Example 1: Long text generation
    print("\n" + "="*50)
    print("Example 1: Long Text Generation")
    print("="*50)
    
    long_prompt = """
    Write a detailed explanation of artificial intelligence, covering:
    1. History and evolution
    2. Current technologies and approaches
    3. Applications in various fields
    4. Future prospects and challenges
    5. Ethical considerations
    
    Make it comprehensive and educational.
    """
    
    result = client.stream_process(long_prompt, max_tokens=800)
    
    if result:
        print(f"\nüìÑ Full response length: {len(result)} characters")
        print(f"üìù First 200 chars: {result[:200]}...")
    
    # Example 2: Creative writing
    print("\n" + "="*50)
    print("Example 2: Creative Writing")
    print("="*50)
    
    creative_prompt = """
    Write a short science fiction story about:
    - An AI that becomes self-aware
    - Set in the year 2045
    - Include themes of consciousness and identity
    - Make it thought-provoking
    - Around 500 words
    """
    
    result = client.stream_process(creative_prompt, max_tokens=600)
    
    if result:
        print(f"\nüìñ Story length: {len(result)} words")
        print(f"‚ú® First 200 chars: {result[:200]}...")

if __name__ == "__main__":
    main()
```

### 5. Batch Processing (advanced/batch_processing.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Batch Processing Example
Process multiple requests efficiently
"""

import requests
import time
import concurrent.futures
from typing import List, Dict

class BatchProcessor:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    def process_batch(self, requests: List[Dict]) -> List[Dict]:
        """Process multiple requests in batch."""
        url = f"{self.base_url}/batch"
        
        print(f"üöÄ Processing batch of {len(requests)} requests...")
        
        try:
            response = requests.post(url, json={"requests": requests})
            
            if response.status_code == 200:
                result = response.json()
                return result.get('results', [])
            else:
                print(f"‚ùå Batch error: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"‚ùå Batch processing error: {e}")
            return []
    
    def process_parallel(self, requests: List[Dict], max_workers: int = 5) -> List[Dict]:
        """Process requests in parallel."""
        url = f"{self.base_url}/process"
        
        def single_request(request_data):
            try:
                response = requests.post(url, json=request_data, timeout=30)
                if response.status_code == 200:
                    return response.json()
                else:
                    return {"success": False, "error": f"HTTP {response.status_code}"}
            except Exception as e:
                return {"success": False, "error": str(e)}
        
        print(f"üîÑ Processing {len(requests)} requests with {max_workers} workers...")
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(single_request, req) for req in requests]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        processing_time = time.time() - start_time
        
        print(f"‚úÖ Parallel processing complete in {processing_time:.2f}s")
        
        return results

def create_test_requests() -> List[Dict]:
    """Create diverse test requests."""
    requests = [
        # Text generation
        {
            "input_data": "What is machine learning?",
            "task_type": "text_generation",
            "max_tokens": 100
        },
        {
            "input_data": "Explain quantum computing",
            "task_type": "text_generation",
            "max_tokens": 150
        },
        
        # Neural processing
        {
            "input_data": [1.0, 2.0, 3.0, 4.0, 5.0] * 200,
            "task_type": "neural_processing"
        },
        {
            "input_data": [0.5, -0.3, 0.8, -0.2] * 250,
            "task_type": "neural_processing"
        },
        
        # Creative tasks
        {
            "input_data": "Write a haiku about technology",
            "task_type": "text_generation",
            "max_tokens": 50,
            "temperature": 0.9
        },
        {
            "input_data": "Create a slogan for an AI company",
            "task_type": "text_generation",
            "max_tokens": 30,
            "temperature": 0.8
        },
        
        # Analysis tasks
        {
            "input_data": "Analyze the sentiment: 'I love this product!'",
            "task_type": "text_analysis"
        },
        {
            "input_data": "Summarize: AI is transforming healthcare",
            "task_type": "text_analysis"
        }
    ]
    
    return requests

def analyze_results(results: List[Dict]):
    """Analyze processing results."""
    total_requests = len(results)
    successful_requests = sum(1 for r in results if r.get('success', False))
    failed_requests = total_requests - successful_requests
    
    print(f"\nüìä Batch Analysis:")
    print(f"   Total requests: {total_requests}")
    print(f"   ‚úÖ Successful: {successful_requests}")
    print(f"   ‚ùå Failed: {failed_requests}")
    print(f"   üìà Success rate: {successful_requests/total_requests*100:.1f}%")
    
    # Analyze processing times
    processing_times = [
        r.get('metadata', {}).get('processing_time', 0) 
        for r in results if r.get('success', False)
    ]
    
    if processing_times:
        avg_time = sum(processing_times) / len(processing_times)
        min_time = min(processing_times)
        max_time = max(processing_times)
        
        print(f"\n‚è±Ô∏è Processing Times:")
        print(f"   Average: {avg_time:.3f}s")
        print(f"   Minimum: {min_time:.3f}s")
        print(f"   Maximum: {max_time:.3f}s")
    
    # Analyze modules used
    all_modules = []
    for r in results:
        modules = r.get('metadata', {}).get('modules_used', [])
        all_modules.extend(modules)
    
    if all_modules:
        from collections import Counter
        module_counts = Counter(all_modules)
        
        print(f"\nüîß Modules Used:")
        for module, count in module_counts.most_common():
            print(f"   {module}: {count} times")

def main():
    print("üì¶ SYNTARA-PRO Batch Processing Example")
    
    # Create test requests
    test_requests = create_test_requests()
    
    processor = BatchProcessor()
    
    # Method 1: Batch endpoint
    print("\n" + "="*50)
    print("Method 1: Batch Endpoint")
    print("="*50)
    
    batch_results = processor.process_batch(test_requests)
    analyze_results(batch_results)
    
    # Method 2: Parallel processing
    print("\n" + "="*50)
    print("Method 2: Parallel Processing")
    print("="*50)
    
    parallel_results = processor.process_parallel(test_requests, max_workers=3)
    analyze_results(parallel_results)
    
    # Comparison
    print("\n" + "="*50)
    print("Comparison")
    print("="*50)
    
    batch_success = sum(1 for r in batch_results if r.get('success', False))
    parallel_success = sum(1 for r in parallel_results if r.get('success', False))
    
    print(f"Batch endpoint success rate: {batch_success/len(test_requests)*100:.1f}%")
    print(f"Parallel processing success rate: {parallel_success/len(test_requests)*100:.1f}%")

if __name__ == "__main__":
    main()
```

---

## Bilingual Examples

### 6. Hindi Support (bilingual/hindi_support.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Hindi Language Support Example
Demonstrate Hindi text processing capabilities
"""

import requests
import json

def hindi_text_examples():
    """Examples of Hindi text processing."""
    return [
        {
            "name": "Basic Greeting",
            "text": "‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ! ‡§Æ‡•à‡§Ç SYNTARA-PRO ‡§π‡•Ç‡§Å‡•§",
            "expected_type": "text_generation"
        },
        {
            "name": "Question",
            "text": "‡§ï‡•É‡§™‡§∑‡•ç‡§£‡§æ ‡§¨‡§§‡§æ‡§á‡§è ‡§ï‡§ø ‡§Ü‡§∞‡•ç‡§ü‡§ø‡§´‡§ø‡§∂‡§ø‡§Ø‡§≤ ‡§á‡§Ç‡§ü‡•á‡§≤‡§ø‡§ú‡•á‡§Ç‡§∏ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?",
            "expected_type": "text_generation"
        },
        {
            "name": "Creative Writing",
            "text": "‡§è‡§ï ‡§ï‡§π‡§æ‡§®‡•Ä ‡§≤‡§ø‡§ñ‡•á‡§Ç ‡§ú‡•ã ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡•á ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§ ‡§π‡•ã",
            "expected_type": "text_generation",
            "temperature": 0.8,
            "max_tokens": 200
        },
        {
            "name": "Technical Explanation",
            "text": "‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§ï‡•Ä ‡§¨‡•Å‡§®‡§ø‡§Ø‡§æ‡§¶‡•Ä ‡§∏‡§Æ‡§ù‡§æ‡§á‡§è",
            "expected_type": "text_generation",
            "max_tokens": 300
        },
        {
            "name": "Poetry",
            "text": "‡§™‡•ç‡§∞‡•á‡§Æ ‡§î‡§∞ ‡§§‡§ï‡§®‡•Ä‡§ï ‡§™‡§∞ ‡§è‡§ï ‡§ï‡§µ‡§ø‡§§‡§æ ‡§≤‡§ø‡§ñ‡•á‡§Ç",
            "expected_type": "text_generation",
            "temperature": 0.9,
            "max_tokens": 150
        }
    ]

def process_hindi_text(text, task_type="text_generation", **kwargs):
    """Process Hindi text with SYNTARA-PRO."""
    url = "http://localhost:8000/process"
    
    payload = {
        "input_data": text,
        "task_type": task_type,
        **kwargs
    }
    
    try:
        response = requests.post(url, json=payload)
        if response.status_code == 200:
            return response.json()
        else:
            return {"success": False, "error": f"HTTP {response.status_code}"}
    except Exception as e:
        return {"success": False, "error": str(e)}

def main():
    print("üáÆüá≥ SYNTARA-PRO Hindi Language Support Example")
    print("="*60)
    
    examples = hindi_text_examples()
    
    for i, example in enumerate(examples, 1):
        print(f"\n{i}. {example['name']}")
        print("-" * 40)
        print(f"üìù Input: {example['text']}")
        
        # Process the text
        result = process_hindi_text(
            example['text'],
            example['expected_type'],
            **{k: v for k, v in example.items() 
               if k not in ['name', 'text', 'expected_type']}
        )
        
        if result.get('success', False):
            print(f"‚úÖ Success!")
            print(f"ü§ñ Response: {result.get('result', '')[:200]}...")
            print(f"üîß Modules: {result.get('metadata', {}).get('modules_used', [])}")
            print(f"‚è±Ô∏è Time: {result.get('metadata', {}).get('processing_time', 0):.3f}s")
        else:
            print(f"‚ùå Error: {result.get('error', 'Unknown error')}")
        
        print()

def mixed_language_example():
    """Example of mixed language processing."""
    print("\n" + "="*60)
    print("Mixed Language Example (Hinglish)")
    print("="*60)
    
    mixed_texts = [
        "Hello ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç, how are you aaj kal?",
        "Mujhe AI ke baare mein jaanna hai",
        "Technology ‡§¨‡§π‡•Å‡§§ ‡§ú‡§º‡§∞‡•Ç‡§∞‡•Ä badh rahi hai",
        "Let's discuss about ‡§≠‡§µ‡§ø‡§∑‡•ç‡§Ø ‡§ï‡§æ future"
    ]
    
    for text in mixed_texts:
        print(f"\nüìù Mixed: {text}")
        result = process_hindi_text(text, "text_generation")
        
        if result.get('success', False):
            print(f"ü§ñ Response: {result.get('result', '')[:100]}...")

if __name__ == "__main__":
    main()
    mixed_language_example()
```

---

## Production Examples

### 7. Error Handling (production/error_handling.py)
```python
#!/usr/bin/env python3
"""
SYNTARA-PRO Production Error Handling Example
Robust error handling and retry logic
"""

import requests
import time
import json
import logging
from typing import Optional, Dict, Any
from enum import Enum

class ErrorType(Enum):
    NETWORK_ERROR = "network_error"
    API_ERROR = "api_error"
    RATE_LIMIT = "rate_limit"
    TIMEOUT = "timeout"
    SERVER_ERROR = "server_error"

class SyntaraProClient:
    def __init__(self, base_url="http://localhost:8000", api_key=None):
        self.base_url = base_url
        self.api_key = api_key
        self.max_retries = 3
        self.retry_delay = 1.0
        self.timeout = 30
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def make_request(self, endpoint: str, payload: Dict, 
                   retries: int = None) -> Optional[Dict]:
        """Make robust API request with retry logic."""
        if retries is None:
            retries = self.max_retries
        
        url = f"{self.base_url}{endpoint}"
        headers = {}
        
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        for attempt in range(retries + 1):
            try:
                self.logger.info(f"Attempt {attempt + 1}/{retries + 1}: {endpoint}")
                
                response = requests.post(
                    url,
                    json=payload,
                    headers=headers,
                    timeout=self.timeout
                )
                
                # Handle different HTTP status codes
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 429:
                    self.logger.warning("Rate limit hit, implementing backoff")
                    time.sleep(self.retry_delay * (2 ** attempt))
                    continue
                elif response.status_code >= 500:
                    self.logger.warning(f"Server error: {response.status_code}")
                    if attempt < retries:
                        time.sleep(self.retry_delay * (2 ** attempt))
                        continue
                    else:
                        return self.create_error_response(
                            ErrorType.SERVER_ERROR,
                            f"Server error after {retries} retries"
                        )
                else:
                    return self.create_error_response(
                        ErrorType.API_ERROR,
                        f"HTTP {response.status_code}: {response.text}"
                    )
                    
            except requests.exceptions.Timeout:
                self.logger.warning(f"Timeout on attempt {attempt + 1}")
                if attempt < retries:
                    time.sleep(self.retry_delay * (2 ** attempt))
                    continue
                else:
                    return self.create_error_response(
                        ErrorType.TIMEOUT,
                        f"Timeout after {retries} retries"
                    )
                    
            except requests.exceptions.ConnectionError:
                self.logger.warning(f"Connection error on attempt {attempt + 1}")
                if attempt < retries:
                    time.sleep(self.retry_delay * (2 ** attempt))
                    continue
                else:
                    return self.create_error_response(
                        ErrorType.NETWORK_ERROR,
                        f"Connection failed after {retries} retries"
                    )
                    
            except Exception as e:
                self.logger.error(f"Unexpected error: {e}")
                return self.create_error_response(
                    ErrorType.API_ERROR,
                    f"Unexpected error: {str(e)}"
                )
        
        return None
    
    def create_error_response(self, error_type: ErrorType, message: str) -> Dict:
        """Create standardized error response."""
        return {
            "success": False,
            "error": message,
            "error_type": error_type.value,
            "timestamp": time.time()
        }
    
    def process_with_fallback(self, input_data: str, 
                           primary_task: str = "text_generation",
                           fallback_task: str = "text_analysis") -> Dict:
        """Process with fallback task type."""
        
        # Try primary task
        result = self.make_request("/process", {
            "input_data": input_data,
            "task_type": primary_task
        })
        
        if result and result.get('success', False):
            return result
        
        # Try fallback task
        self.logger.info(f"Primary task failed, trying fallback: {fallback_task}")
        fallback_result = self.make_request("/process", {
            "input_data": input_data,
            "task_type": fallback_task
        })
        
        if fallback_result and fallback_result.get('success', False):
            fallback_result['fallback_used'] = True
            fallback_result['original_task'] = primary_task
            return fallback_result
        
        # Both failed
        return self.create_error_response(
            ErrorType.API_ERROR,
            f"Both primary and fallback tasks failed"
        )

def demonstrate_error_handling():
    """Demonstrate various error scenarios."""
    client = SyntaraProClient(api_key="test-key")
    
    print("üõ°Ô∏è SYNTARA-PRO Error Handling Examples")
    print("="*50)
    
    # Example 1: Normal request
    print("\n1. Normal Request")
    result = client.make_request("/process", {
        "input_data": "Hello world",
        "task_type": "text_generation"
    })
    
    if result:
        print(f"‚úÖ Success: {result.get('success', False)}")
        if not result.get('success', False):
            print(f"‚ùå Error: {result.get('error', 'Unknown')}")
    
    # Example 2: Invalid endpoint
    print("\n2. Invalid Endpoint")
    result = client.make_request("/invalid", {
        "input_data": "test"
    })
    
    if result:
        print(f"‚úÖ Success: {result.get('success', False)}")
        if not result.get('success', False):
            print(f"‚ùå Error: {result.get('error', 'Unknown')}")
    
    # Example 3: Fallback mechanism
    print("\n3. Fallback Mechanism")
    result = client.process_with_fallback(
        "Complex input that might fail",
        primary_task="invalid_task",
        fallback_task="text_analysis"
    )
    
    if result:
        print(f"‚úÖ Success: {result.get('success', False)}")
        print(f"üîÑ Fallback used: {result.get('fallback_used', False)}")
        if result.get('success', False):
            print(f"‚ùå Error: {result.get('error', 'Unknown')}")

def main():
    demonstrate_error_handling()

if __name__ == "__main__":
    main()
```

---

## Running Examples

### Prerequisites
```bash
# Install required packages
pip install requests numpy matplotlib pillow

# Start SYNTARA-PRO server
python syntara_pro_server.py
```

### Run Examples
```bash
# Basic examples
python basic/hello_world.py
python basic/neural_processing.py
python basic/vision_analysis.py

# Advanced examples
python advanced/streaming_client.py
python advanced/batch_processing.py

# Bilingual examples
python bilingual/hindi_support.py

# Production examples
python production/error_handling.py
```

---

## Expected Output

Each example will demonstrate different aspects of SYNTARA-PRO:

1. **Hello World**: Basic text generation
2. **Neural Processing**: Spiking network analysis
3. **Vision Analysis**: Image feature extraction
4. **Streaming**: Real-time response streaming
5. **Batch Processing**: Efficient bulk processing
6. **Hindi Support**: Multilingual capabilities
7. **Error Handling**: Robust production code

---

*For more examples, check the examples directory!*
